---
title: Schedule (subject to change as the term progresses)
---

  
Aug 26
: [Training language models to follow instructions with human feedback](https://cocoxu.github.io/CS8803-LLM-fall2024/slides/01_RLHF_InstructGPT.pdf)
  : [PPO/InstructGPT](https://arxiv.org/abs/2203.02155)
  
Aug 28
: [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://cocoxu.github.io/CS8803-LLM-fall2024/slides/02_DPO.pdf)
  : [DPO](https://arxiv.org/abs/2305.18290), [Unpacking DPO/PPO](https://arxiv.org/abs/2406.09279)